{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 2254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed for np.random\n",
    "seed=9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling imported original dataset\n",
    "np.random.shuffle(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset with (Seed 9) :\n",
      "\n",
      "[[3.9640e+03 6.0000e+00 4.0000e+00 ... 1.0000e+00 5.7600e+02 0.0000e+00]\n",
      " [3.9104e+04 7.0000e+00 7.0000e+00 ... 2.0000e+00 4.3900e+02 1.0000e+00]\n",
      " [6.0400e+03 4.0000e+00 5.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [8.7770e+03 5.0000e+00 7.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.4480e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 4.7400e+02 0.0000e+00]\n",
      " [9.2450e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 6.3900e+02 1.0000e+00]]\n",
      "\n",
      "(1460, 11)\n"
     ]
    }
   ],
   "source": [
    "#Shuffled dataset\n",
    "print(\"Shuffled dataset with (Seed \"+str(seed) +\") :\\n\\n\"+str(data_orig))\n",
    "print(\"\\n\"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[0. 1. 0. ... 0. 0. 1.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n"
     ]
    }
   ],
   "source": [
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : \n",
      "\n",
      "[[3.9640e+03 3.9104e+04 6.0400e+03 ... 8.7770e+03 2.4480e+03 9.2450e+03]\n",
      " [6.0000e+00 7.0000e+00 4.0000e+00 ... 5.0000e+00 7.0000e+00 7.0000e+00]\n",
      " [4.0000e+00 7.0000e+00 5.0000e+00 ... 7.0000e+00 5.0000e+00 5.0000e+00]\n",
      " ...\n",
      " [1.0000e+01 5.0000e+00 6.0000e+00 ... 4.0000e+00 6.0000e+00 8.0000e+00]\n",
      " [1.0000e+00 2.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [5.7600e+02 4.3900e+02 0.0000e+00 ... 0.0000e+00 4.7400e+02 6.3900e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "x_shuffled = data_orig[:,0:-1].T\n",
    "print(\"Input set : \\n\\n\" +str(x_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "print(x_shuffled.shape)\n",
    "X=x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed of Randomization   : 9\n",
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"Seed of Randomization   : \"+str(seed))\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"\n",
    "    Input  :  Numpy array x \n",
    "    Output :  Numpy array of same shape as X but standardized along each rows\n",
    "    \n",
    "    \"\"\"\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    #print(\"Mean of each row : \\n\\n\"+str(x_mean))\n",
    "    #print(\"\\nStandard deviation of each row : \\n\\n\"+str(x_std))\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize train_X : (10, 1168)\n",
      "\n",
      "[[-0.62294439  2.64369886 -0.42995773 ... -0.07112896  0.33101956\n",
      "  -0.14224398]\n",
      " [-0.06530134  0.66109837 -1.51810077 ...  0.66109837  1.38749808\n",
      "  -0.06530134]\n",
      " [-1.39247961  1.28548337 -0.49982528 ... -0.49982528 -0.49982528\n",
      "  -0.49982528]\n",
      " ...\n",
      " [ 2.17516164 -0.92614304 -0.30588211 ...  1.5549007   1.5549007\n",
      "   0.31437883]\n",
      " [ 0.59351649  2.15836359 -0.9713306  ...  0.59351649  0.59351649\n",
      "  -0.9713306 ]\n",
      " [ 0.48109632 -0.16278512 -2.22602564 ...  0.85708548  1.9803531\n",
      "   0.33070066]]\n",
      "\n",
      "\n",
      "Standardize test_X : (10, 292)\n",
      "\n",
      "[[-0.09938328  0.03686526 -0.32759958 ... -0.19526819 -1.27316445\n",
      "  -0.11556279]\n",
      " [-1.52084774 -0.09749024 -0.09749024 ... -0.80916899  0.61418851\n",
      "   0.61418851]\n",
      " [ 0.33665213 -0.59072921 -0.59072921 ...  1.26403347 -0.59072921\n",
      "  -0.59072921]\n",
      " ...\n",
      " [-1.56607354  0.22958146  0.82813313 ... -1.56607354 -0.36897021\n",
      "   0.82813313]\n",
      " [-0.87582323  0.62853197  0.62853197 ... -0.87582323 -0.87582323\n",
      "  -0.87582323]\n",
      " [-1.08665717  0.03514285 -0.18094158 ... -2.16248178  0.01675268\n",
      "   0.77534696]]\n"
     ]
    }
   ],
   "source": [
    "train_X = standardize(train_X)\n",
    "print(\"Standardize train_X : \"+str(train_X.shape)+\"\\n\\n\"+str(train_X))\n",
    "test_X  = standardize(test_X)\n",
    "print(\"\\n\\nStandardize test_X : \"+str(test_X.shape)+\"\\n\\n\"+str(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigz= 1/(1+np.exp(-Z))\n",
    "    sigz[sigz==1] = 0.99999999999\n",
    "    sigz[sigz==0] = 0.000000000001\n",
    "    return sigz        \n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.1\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(2/layer_dims[l-1])**0.5\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    #Z = standardize(Z) Batch-Normalize with u,var=1\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation,layer):\n",
    "    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "        dropout_cache = A\n",
    "        D = np.random.rand(A.shape[0],A.shape[1]) \n",
    "        if layer==1:\n",
    "            D[:,:]=1\n",
    "        else:\n",
    "            D = (D < keep_prob).astype(int)                                         \n",
    "            A = A*D                                         \n",
    "            A = A/keep_prob \n",
    "        global Dcache \n",
    "        Dcache = D\n",
    "    \n",
    "    cache = (linear_cache, activation_cache,Dcache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    D = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)],\"relu\",l)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)],\"sigmoid\",l)\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y,parameters):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    sumW = 0\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(1, L):\n",
    "        sumW= sumW + np.sum(parameters[\"W\"+str(l)])\n",
    "        \n",
    "    L2_cost= lambd*(sumW)/(2*m)\n",
    "    cost = cost + L2_cost\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache,keep_prob):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation,keep_prob):\n",
    "\n",
    "    linear_cache, activation_cache, dropout_cache = cache\n",
    "    global dA_prev, dW, db\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob=1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "   \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t,m, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8,):\n",
    "\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+ str(l+1)] +(lambd/m)*parameters[\"W\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.003, num_iterations = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_after=1):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 10000000\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y,parameters)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        t = t + 0.1\n",
    "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t,m, learning_rate, beta1, beta2,  epsilon,)\n",
    "        if i % print_after == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if  i % print_after == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per '+str(print_after)+')')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    #print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    error = (1-acc)*100\n",
    "    print(Set+\" :       \"+'%0.3f'%error+\" %\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.801057\n",
      "Cost after iteration 100: 0.389741\n",
      "Cost after iteration 200: 0.255491\n",
      "Cost after iteration 300: 0.218642\n",
      "Cost after iteration 400: 0.205511\n",
      "Cost after iteration 500: 0.203038\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :       8.904 %\n",
      "Test  :       8.904 %\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "global dropout_cache\n",
    "global keep_prob\n",
    "global lambd\n",
    "\n",
    "keep_prob=1\n",
    "lambd=0.05\n",
    "\n",
    "parameters = model(train_X, train_Y, layers_dims = [10,65,50,25,15,10,5,1], num_iterations =600, \n",
    "                           learning_rate = 0.000085,  beta1 = 0.92, beta2 = 0.99,  epsilon = 1e-8, \n",
    "                           print_after = 100)\n",
    "\n",
    "def predict(X,parameters):\n",
    "    keep_prob=1\n",
    "    AL = forwardprop(X, parameters)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,parameters)\n",
    "train_Yhat = predict(train_X,parameters)\n",
    "\n",
    "\n",
    "#print(\"    \"+\" :       \"+ \"\\t Precision \" + \"  \"+ \"     \\tRecall\" +\"  \"+\"          F-score \"+\"  \"+\"         Accuracy\")\n",
    "\n",
    "evaluate(train_Y,train_Yhat,\"Train\")\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #print(caches[-2][-1].shape)\n",
    "    #print(L)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\",keep_prob=1)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        global Dprev_cache\n",
    "        D_prev = caches[l-1][2]\n",
    "        global dA_prev_temp, dW_temp, db_temp\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                \"relu\",keep_prob)\n",
    "        if l > 0:\n",
    "            dA_prev_temp = np.multiply(dA_prev_temp,D_prev)\n",
    "            dA_prev_temp = dA_prev_temp/keep_prob\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
