{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1974,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 1974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1975,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1976,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed for np.random\n",
    "seed=9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1977,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling imported original dataset\n",
    "np.random.shuffle(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1978,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset with (Seed 9) :\n",
      "\n",
      "[[3.9640e+03 6.0000e+00 4.0000e+00 ... 1.0000e+00 5.7600e+02 0.0000e+00]\n",
      " [3.9104e+04 7.0000e+00 7.0000e+00 ... 2.0000e+00 4.3900e+02 1.0000e+00]\n",
      " [6.0400e+03 4.0000e+00 5.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [8.7770e+03 5.0000e+00 7.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.4480e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 4.7400e+02 0.0000e+00]\n",
      " [9.2450e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 6.3900e+02 1.0000e+00]]\n",
      "\n",
      "(1460, 11)\n"
     ]
    }
   ],
   "source": [
    "#Shuffled dataset\n",
    "print(\"Shuffled dataset with (Seed \"+str(seed) +\") :\\n\\n\"+str(data_orig))\n",
    "print(\"\\n\"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1979,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[0. 1. 0. ... 0. 0. 1.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n"
     ]
    }
   ],
   "source": [
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1981,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : \n",
      "\n",
      "[[3.9640e+03 3.9104e+04 6.0400e+03 ... 8.7770e+03 2.4480e+03 9.2450e+03]\n",
      " [6.0000e+00 7.0000e+00 4.0000e+00 ... 5.0000e+00 7.0000e+00 7.0000e+00]\n",
      " [4.0000e+00 7.0000e+00 5.0000e+00 ... 7.0000e+00 5.0000e+00 5.0000e+00]\n",
      " ...\n",
      " [1.0000e+01 5.0000e+00 6.0000e+00 ... 4.0000e+00 6.0000e+00 8.0000e+00]\n",
      " [1.0000e+00 2.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [5.7600e+02 4.3900e+02 0.0000e+00 ... 0.0000e+00 4.7400e+02 6.3900e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "x_shuffled = data_orig[:,0:-1].T\n",
    "print(\"Input set : \\n\\n\" +str(x_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1982,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "print(x_shuffled.shape)\n",
    "X=x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1983,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed of Randomization   : 9\n",
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"Seed of Randomization   : \"+str(seed))\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1985,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"\n",
    "    Input  :  Numpy array x \n",
    "    Output :  Numpy array of same shape as X but standardized along each rows\n",
    "    \n",
    "    \"\"\"\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    #print(\"Mean of each row : \\n\\n\"+str(x_mean))\n",
    "    #print(\"\\nStandard deviation of each row : \\n\\n\"+str(x_std))\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1986,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize train_X : (10, 1168)\n",
      "\n",
      "[[-0.62294439  2.64369886 -0.42995773 ... -0.07112896  0.33101956\n",
      "  -0.14224398]\n",
      " [-0.06530134  0.66109837 -1.51810077 ...  0.66109837  1.38749808\n",
      "  -0.06530134]\n",
      " [-1.39247961  1.28548337 -0.49982528 ... -0.49982528 -0.49982528\n",
      "  -0.49982528]\n",
      " ...\n",
      " [ 2.17516164 -0.92614304 -0.30588211 ...  1.5549007   1.5549007\n",
      "   0.31437883]\n",
      " [ 0.59351649  2.15836359 -0.9713306  ...  0.59351649  0.59351649\n",
      "  -0.9713306 ]\n",
      " [ 0.48109632 -0.16278512 -2.22602564 ...  0.85708548  1.9803531\n",
      "   0.33070066]]\n",
      "\n",
      "\n",
      "Standardize test_X : (10, 292)\n",
      "\n",
      "[[-0.09938328  0.03686526 -0.32759958 ... -0.19526819 -1.27316445\n",
      "  -0.11556279]\n",
      " [-1.52084774 -0.09749024 -0.09749024 ... -0.80916899  0.61418851\n",
      "   0.61418851]\n",
      " [ 0.33665213 -0.59072921 -0.59072921 ...  1.26403347 -0.59072921\n",
      "  -0.59072921]\n",
      " ...\n",
      " [-1.56607354  0.22958146  0.82813313 ... -1.56607354 -0.36897021\n",
      "   0.82813313]\n",
      " [-0.87582323  0.62853197  0.62853197 ... -0.87582323 -0.87582323\n",
      "  -0.87582323]\n",
      " [-1.08665717  0.03514285 -0.18094158 ... -2.16248178  0.01675268\n",
      "   0.77534696]]\n"
     ]
    }
   ],
   "source": [
    "train_X = standardize(train_X)\n",
    "print(\"Standardize train_X : \"+str(train_X.shape)+\"\\n\\n\"+str(train_X))\n",
    "test_X  = standardize(test_X)\n",
    "print(\"\\n\\nStandardize test_X : \"+str(test_X.shape)+\"\\n\\n\"+str(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1987,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigz= 1/(1+np.exp(-Z))\n",
    "    sigz[sigz==1] = 0.99999999999\n",
    "    sigz[sigz==0] = 0.000000000001\n",
    "    return sigz        \n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1988,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.1\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(2/layer_dims[l-1])**0.5\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1990,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    #Z = standardize(Z) Batch-Normalize with u,var=1\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1991,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "        dropout_cache = A\n",
    "        D = np.random.rand(A.shape[0],A.shape[1])                                        \n",
    "        D = (D < keep_prob).astype(int)                                         \n",
    "        A = A*D                                         \n",
    "        A = A/keep_prob \n",
    "        global Dcache \n",
    "        Dcache = D\n",
    "    \n",
    "    cache = (linear_cache, activation_cache,Dcache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2005,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    D = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1993,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1994,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache,keep_prob):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1995,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation,keep_prob):\n",
    "\n",
    "    linear_cache, activation_cache, dropout_cache = cache\n",
    "    global dA_prev, dW, db\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob=1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1996,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1997,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "   \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1998,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1999,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.003, num_iterations = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_after=1):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 10000000\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        t = t + 0.1\n",
    "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        if i % print_after == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if  i % print_after == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per '+str(print_after)+')')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2000,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    #print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    error = (1-acc)*100\n",
    "    print(Set+\" :       \"+'%0.3f'%error+\" %\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2015,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.858608\n",
      "Cost after iteration 100: 0.449797\n",
      "Cost after iteration 200: 0.275363\n",
      "Cost after iteration 300: 0.223139\n",
      "Cost after iteration 400: 0.206983\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :       8.904 %\n",
      "Test  :       10.959 %\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "global dropout_cache\n",
    "global keep_prob\n",
    "keep_prob=1\n",
    "parameters = model(train_X, train_Y, layers_dims = [10,64,48,24,16,8,4,1], num_iterations =401, \n",
    "                           learning_rate = 0.000085,  beta1 = 0.92, beta2 = 0.9,  epsilon = 1e-8, \n",
    "                           print_after = 100)\n",
    "\n",
    "def predict(X,parameters):\n",
    "    \n",
    "    AL = forwardprop(X, parameters)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,parameters)\n",
    "train_Yhat = predict(train_X,parameters)\n",
    "\n",
    "\n",
    "#print(\"    \"+\" :       \"+ \"\\t Precision \" + \"  \"+ \"     \\tRecall\" +\"  \"+\"          F-score \"+\"  \"+\"         Accuracy\")\n",
    "\n",
    "evaluate(train_Y,train_Yhat,\"Train\")\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2002,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #print(caches[-2][-1].shape)\n",
    "    #print(L)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\",keep_prob=1)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        global Dprev_cache\n",
    "        D_prev = caches[l-1][2]\n",
    "        global dA_prev_temp, dW_temp, db_temp\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                \"relu\",keep_prob)\n",
    "        if l > 0:\n",
    "            dA_prev_temp = np.multiply(dA_prev_temp,D_prev)\n",
    "            dA_prev_temp = dA_prev_temp/keep_prob\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
