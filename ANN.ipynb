{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[1. 1. 1. ... 1. 0. 0.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n",
      "49.86301369863014\n"
     ]
    }
   ],
   "source": [
    "#Removing Rank 1 array\n",
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))\n",
    "print((np.sum(Y)/1460)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "X = data_orig[:,0:-1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))\n",
    "print((np.sum(1-test_Y)/292)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = standardize(train_X)\n",
    "test_X  = standardize(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigz= 1/(1+np.exp(-Z))\n",
    "    sigz[sigz==1] = 0.99999999999\n",
    "    sigz[sigz==0] = 0.000000000001\n",
    "    return sigz        \n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.1\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    p = {\"W1\": W1,\"b1\": b1,   \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(2/layer_dims[l-1])**0.5\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    #Z = standardize(Z) Batch-Normalize with u,var=1\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation,layer):\n",
    "    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "        dropout_cache = A\n",
    "        D = np.random.rand(A.shape[0],A.shape[1]) \n",
    "        if layer==1:\n",
    "            D[:,:]=1\n",
    "        else:\n",
    "            D = (D < keep_prob).astype(int)                                         \n",
    "            A = A*D                                         \n",
    "            A = A/keep_prob \n",
    "        global Dcache \n",
    "        Dcache = D\n",
    "    \n",
    "    cache = (linear_cache, activation_cache,Dcache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    D = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)],\"relu\",l)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)],\"sigmoid\",l)\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y,parameters):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    sumW = 0\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(1, L):\n",
    "        sumW= sumW + np.sum(parameters[\"W\"+str(l)])\n",
    "        \n",
    "    L2_cost= lambd*(sumW)/(2*m)\n",
    "    cost = cost + L2_cost\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache,keep_prob):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation,keep_prob):\n",
    "\n",
    "    linear_cache, activation_cache, dropout_cache = cache\n",
    "    global dA_prev, dW, db\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob=1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #print(caches[-2][-1].shape)\n",
    "    #print(L)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\",keep_prob=1)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        global Dprev_cache\n",
    "        D_prev = caches[l-1][2]\n",
    "        global dA_prev_temp, dW_temp, db_temp\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                \"relu\",keep_prob)\n",
    "        if l > 0:\n",
    "            dA_prev_temp = np.multiply(dA_prev_temp,D_prev)\n",
    "            dA_prev_temp = dA_prev_temp/keep_prob\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "   \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t,m, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8,):\n",
    "\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+ str(l+1)] +(lambd/m)*parameters[\"W\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate0 = 0.003, epocs = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_after=1):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    t = 0\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    for i in range(0, epocs):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y,parameters)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        \n",
    "        t = t + 1\n",
    "        learning_rate = learning_rate0/(1+decay_rate*i)\n",
    "        \n",
    "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t,m, learning_rate, beta1, beta2,  epsilon,)\n",
    "        if i % print_after == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if  i % print_after == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per '+str(print_after)+')')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    #print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    error = (1-acc)*100\n",
    "    print(Set+\" :       \"+'%0.3f'%error+\" %\" +'\\t'+str(f1score))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.029440\n",
      "Cost after iteration 50: 0.740801\n",
      "Cost after iteration 100: 0.560957\n",
      "Cost after iteration 150: 0.449812\n",
      "Cost after iteration 200: 0.371837\n",
      "Cost after iteration 250: 0.314062\n",
      "Cost after iteration 300: 0.273572\n",
      "Cost after iteration 350: 0.247158\n",
      "Cost after iteration 400: 0.231971\n",
      "Cost after iteration 450: 0.225030\n",
      "Cost after iteration 500: 0.220656\n",
      "Cost after iteration 550: 0.219294\n",
      "Cost after iteration 600: 0.218026\n",
      "Cost after iteration 650: 0.218277\n",
      "Cost after iteration 700: 0.219704\n",
      "Cost after iteration 750: 0.222219\n",
      "Cost after iteration 800: 0.224290\n",
      "Cost after iteration 850: 0.226000\n",
      "Cost after iteration 900: 0.227800\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :       9.589 %\t0.9031141868512111\n",
      "Test  :       7.877 %\t0.9204152249134948\n"
     ]
    }
   ],
   "source": [
    "global dropout_cache\n",
    "global keep_prob\n",
    "global lambd\n",
    "\n",
    "keep_prob=1\n",
    "lambd=0\n",
    "decay_rate=0\n",
    "\n",
    "p = model(train_X, train_Y, layers_dims = [10,64,32,16,8,4,1], epocs =901, \n",
    "                           learning_rate0 = 0.000088,  beta1 = 0.9, beta2 = 0.9,  epsilon = 1e-8, \n",
    "                           print_after = 50)\n",
    "\n",
    "def predict(X,p):\n",
    "    keep_prob=1\n",
    "    AL = forwardprop(X, p)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,p)\n",
    "train_Yhat = predict(train_X,p)\n",
    "\n",
    "\n",
    "#print(\"    \"+\" :       \"+ \"\\t Precision \" + \"  \"+ \"     \\tRecall\" +\"  \"+\"          F-score \"+\"  \"+\"         Accuracy\")\n",
    "\n",
    "evaluate(train_Y,train_Yhat,\"Train\")\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
