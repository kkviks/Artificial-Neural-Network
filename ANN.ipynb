{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed for np.random\n",
    "seed=9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling imported original dataset\n",
    "np.random.shuffle(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset with (Seed 9) :\n",
      "\n",
      "[[3.9640e+03 6.0000e+00 4.0000e+00 ... 1.0000e+00 5.7600e+02 0.0000e+00]\n",
      " [3.9104e+04 7.0000e+00 7.0000e+00 ... 2.0000e+00 4.3900e+02 1.0000e+00]\n",
      " [6.0400e+03 4.0000e+00 5.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [8.7770e+03 5.0000e+00 7.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.4480e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 4.7400e+02 0.0000e+00]\n",
      " [9.2450e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 6.3900e+02 1.0000e+00]]\n",
      "\n",
      "(1460, 11)\n"
     ]
    }
   ],
   "source": [
    "#Shuffled dataset\n",
    "print(\"Shuffled dataset with (Seed \"+str(seed) +\") :\\n\\n\"+str(data_orig))\n",
    "print(\"\\n\"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[0. 1. 0. ... 0. 0. 1.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n"
     ]
    }
   ],
   "source": [
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : \n",
      "\n",
      "[[3.9640e+03 3.9104e+04 6.0400e+03 ... 8.7770e+03 2.4480e+03 9.2450e+03]\n",
      " [6.0000e+00 7.0000e+00 4.0000e+00 ... 5.0000e+00 7.0000e+00 7.0000e+00]\n",
      " [4.0000e+00 7.0000e+00 5.0000e+00 ... 7.0000e+00 5.0000e+00 5.0000e+00]\n",
      " ...\n",
      " [1.0000e+01 5.0000e+00 6.0000e+00 ... 4.0000e+00 6.0000e+00 8.0000e+00]\n",
      " [1.0000e+00 2.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [5.7600e+02 4.3900e+02 0.0000e+00 ... 0.0000e+00 4.7400e+02 6.3900e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "x_shuffled = data_orig[:,0:-1].T\n",
    "print(\"Input set : \\n\\n\" +str(x_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "print(x_shuffled.shape)\n",
    "X=x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed of Randomization   : 9\n",
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"Seed of Randomization   : \"+str(seed))\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"\n",
    "    Input  :  Numpy array x \n",
    "    Output :  Numpy array of same shape as X but standardized along each rows\n",
    "    \n",
    "    \"\"\"\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    #print(\"Mean of each row : \\n\\n\"+str(x_mean))\n",
    "    #print(\"\\nStandard deviation of each row : \\n\\n\"+str(x_std))\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize train_X : (10, 1168)\n",
      "\n",
      "[[-0.62294439  2.64369886 -0.42995773 ... -0.07112896  0.33101956\n",
      "  -0.14224398]\n",
      " [-0.06530134  0.66109837 -1.51810077 ...  0.66109837  1.38749808\n",
      "  -0.06530134]\n",
      " [-1.39247961  1.28548337 -0.49982528 ... -0.49982528 -0.49982528\n",
      "  -0.49982528]\n",
      " ...\n",
      " [ 2.17516164 -0.92614304 -0.30588211 ...  1.5549007   1.5549007\n",
      "   0.31437883]\n",
      " [ 0.59351649  2.15836359 -0.9713306  ...  0.59351649  0.59351649\n",
      "  -0.9713306 ]\n",
      " [ 0.48109632 -0.16278512 -2.22602564 ...  0.85708548  1.9803531\n",
      "   0.33070066]]\n",
      "\n",
      "\n",
      "Standardize test_X : (10, 292)\n",
      "\n",
      "[[-0.09938328  0.03686526 -0.32759958 ... -0.19526819 -1.27316445\n",
      "  -0.11556279]\n",
      " [-1.52084774 -0.09749024 -0.09749024 ... -0.80916899  0.61418851\n",
      "   0.61418851]\n",
      " [ 0.33665213 -0.59072921 -0.59072921 ...  1.26403347 -0.59072921\n",
      "  -0.59072921]\n",
      " ...\n",
      " [-1.56607354  0.22958146  0.82813313 ... -1.56607354 -0.36897021\n",
      "   0.82813313]\n",
      " [-0.87582323  0.62853197  0.62853197 ... -0.87582323 -0.87582323\n",
      "  -0.87582323]\n",
      " [-1.08665717  0.03514285 -0.18094158 ... -2.16248178  0.01675268\n",
      "   0.77534696]]\n"
     ]
    }
   ],
   "source": [
    "train_X = standardize(train_X)\n",
    "print(\"Standardize train_X : \"+str(train_X.shape)+\"\\n\\n\"+str(train_X))\n",
    "test_X  = standardize(test_X)\n",
    "print(\"\\n\\nStandardize test_X : \"+str(test_X.shape)+\"\\n\\n\"+str(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.1\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(2/layer_dims[l-1])**0.5\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)],activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL+0.0000001)+(1-Y)*np.log(1.000001-AL)))\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost+0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "    \n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.003, num_iterations = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_cost=False):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 10000\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        t = t + 0.01\n",
    "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per thousands)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.713848\n",
      "Cost after iteration 10: 0.699106\n",
      "Cost after iteration 20: 0.679634\n",
      "Cost after iteration 30: 0.662898\n",
      "Cost after iteration 40: 0.646437\n",
      "Cost after iteration 50: 0.629016\n",
      "Cost after iteration 60: 0.610289\n",
      "Cost after iteration 70: 0.589603\n",
      "Cost after iteration 80: 0.568188\n",
      "Cost after iteration 90: 0.546260\n",
      "Cost after iteration 100: 0.524502\n",
      "Cost after iteration 110: 0.503069\n",
      "Cost after iteration 120: 0.482190\n",
      "Cost after iteration 130: 0.462338\n",
      "Cost after iteration 140: 0.443412\n",
      "Cost after iteration 150: 0.425324\n",
      "Cost after iteration 160: 0.407878\n",
      "Cost after iteration 170: 0.391082\n",
      "Cost after iteration 180: 0.375025\n",
      "Cost after iteration 190: 0.359383\n",
      "Cost after iteration 200: 0.344497\n",
      "Cost after iteration 210: 0.330622\n",
      "Cost after iteration 220: 0.318020\n",
      "Cost after iteration 230: 0.306444\n",
      "Cost after iteration 240: 0.295686\n",
      "Cost after iteration 250: 0.285514\n",
      "Cost after iteration 260: 0.276288\n",
      "Cost after iteration 270: 0.268372\n",
      "Cost after iteration 280: 0.261439\n",
      "Cost after iteration 290: 0.255465\n",
      "Cost after iteration 300: 0.250527\n",
      "Cost after iteration 310: 0.246019\n",
      "Cost after iteration 320: 0.241856\n",
      "Cost after iteration 330: 0.237957\n",
      "Cost after iteration 340: 0.234328\n",
      "Cost after iteration 350: 0.231534\n",
      "Cost after iteration 360: 0.229216\n",
      "Cost after iteration 370: 0.227397\n",
      "Cost after iteration 380: 0.225863\n",
      "Cost after iteration 390: 0.224839\n",
      "Cost after iteration 400: 0.223860\n",
      "Cost after iteration 410: 0.222675\n",
      "Cost after iteration 420: 0.221330\n",
      "Cost after iteration 430: 0.219905\n",
      "Cost after iteration 440: 0.218719\n",
      "Cost after iteration 450: 0.217548\n",
      "Cost after iteration 460: 0.216068\n",
      "Cost after iteration 470: 0.215211\n",
      "Cost after iteration 480: 0.214864\n",
      "Cost after iteration 490: 0.214172\n",
      "Cost after iteration 500: 0.214044\n",
      "Cost after iteration 510: 0.213878\n",
      "Cost after iteration 520: 0.213928\n",
      "Cost after iteration 530: 0.213914\n",
      "Cost after iteration 540: 0.213968\n",
      "Cost after iteration 550: 0.213906\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     :       \t Precision        \tRecall            F-score            Accuracy\n",
      "Train :       0.8941368078175895  0.9352640545144804  0.9142381348875936  0.9118150684931506\n",
      "Test  :       0.8666666666666667  0.9219858156028369  0.8934707903780069  0.8938356164383562\n"
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(train_X, train_Y, layers_dims = [10,128,64,32,16,8,4,1], num_iterations =560, \n",
    "                           learning_rate = 0.0001,  beta1 = 0.92, beta2 = 0.9,  epsilon = 1e-8, print_cost = True)\n",
    "\n",
    "def predict(X,parameters):\n",
    "    \n",
    "    AL = L_model_forward(X, parameters)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,parameters)\n",
    "train_Yhat = predict(train_X,parameters)\n",
    "\n",
    "\n",
    "print(\"    \"+\" :       \"+ \"\\t Precision \" + \"  \"+ \"     \\tRecall\" +\"  \"+\"          F-score \"+\"  \"+\"         Accuracy\")\n",
    "\n",
    "evaluate(train_Y,train_Yhat,\"Train\")\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
