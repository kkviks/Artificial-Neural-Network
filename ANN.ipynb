{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 2209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed for np.random\n",
    "seed=9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling imported original dataset\n",
    "np.random.shuffle(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset with (Seed 9) :\n",
      "\n",
      "[[3.9640e+03 6.0000e+00 4.0000e+00 ... 1.0000e+00 5.7600e+02 0.0000e+00]\n",
      " [3.9104e+04 7.0000e+00 7.0000e+00 ... 2.0000e+00 4.3900e+02 1.0000e+00]\n",
      " [6.0400e+03 4.0000e+00 5.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [8.7770e+03 5.0000e+00 7.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.4480e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 4.7400e+02 0.0000e+00]\n",
      " [9.2450e+03 7.0000e+00 5.0000e+00 ... 0.0000e+00 6.3900e+02 1.0000e+00]]\n",
      "\n",
      "(1460, 11)\n"
     ]
    }
   ],
   "source": [
    "#Shuffled dataset\n",
    "print(\"Shuffled dataset with (Seed \"+str(seed) +\") :\\n\\n\"+str(data_orig))\n",
    "print(\"\\n\"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[0. 1. 0. ... 0. 0. 1.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n"
     ]
    }
   ],
   "source": [
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input set : \n",
      "\n",
      "[[3.9640e+03 3.9104e+04 6.0400e+03 ... 8.7770e+03 2.4480e+03 9.2450e+03]\n",
      " [6.0000e+00 7.0000e+00 4.0000e+00 ... 5.0000e+00 7.0000e+00 7.0000e+00]\n",
      " [4.0000e+00 7.0000e+00 5.0000e+00 ... 7.0000e+00 5.0000e+00 5.0000e+00]\n",
      " ...\n",
      " [1.0000e+01 5.0000e+00 6.0000e+00 ... 4.0000e+00 6.0000e+00 8.0000e+00]\n",
      " [1.0000e+00 2.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [5.7600e+02 4.3900e+02 0.0000e+00 ... 0.0000e+00 4.7400e+02 6.3900e+02]]\n"
     ]
    }
   ],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "x_shuffled = data_orig[:,0:-1].T\n",
    "print(\"Input set : \\n\\n\" +str(x_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "print(x_shuffled.shape)\n",
    "X=x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed of Randomization   : 9\n",
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"Seed of Randomization   : \"+str(seed))\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"\n",
    "    Input  :  Numpy array x \n",
    "    Output :  Numpy array of same shape as X but standardized along each rows\n",
    "    \n",
    "    \"\"\"\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    #print(\"Mean of each row : \\n\\n\"+str(x_mean))\n",
    "    #print(\"\\nStandard deviation of each row : \\n\\n\"+str(x_std))\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize train_X : (10, 1168)\n",
      "\n",
      "[[-0.62294439  2.64369886 -0.42995773 ... -0.07112896  0.33101956\n",
      "  -0.14224398]\n",
      " [-0.06530134  0.66109837 -1.51810077 ...  0.66109837  1.38749808\n",
      "  -0.06530134]\n",
      " [-1.39247961  1.28548337 -0.49982528 ... -0.49982528 -0.49982528\n",
      "  -0.49982528]\n",
      " ...\n",
      " [ 2.17516164 -0.92614304 -0.30588211 ...  1.5549007   1.5549007\n",
      "   0.31437883]\n",
      " [ 0.59351649  2.15836359 -0.9713306  ...  0.59351649  0.59351649\n",
      "  -0.9713306 ]\n",
      " [ 0.48109632 -0.16278512 -2.22602564 ...  0.85708548  1.9803531\n",
      "   0.33070066]]\n",
      "\n",
      "\n",
      "Standardize test_X : (10, 292)\n",
      "\n",
      "[[-0.09938328  0.03686526 -0.32759958 ... -0.19526819 -1.27316445\n",
      "  -0.11556279]\n",
      " [-1.52084774 -0.09749024 -0.09749024 ... -0.80916899  0.61418851\n",
      "   0.61418851]\n",
      " [ 0.33665213 -0.59072921 -0.59072921 ...  1.26403347 -0.59072921\n",
      "  -0.59072921]\n",
      " ...\n",
      " [-1.56607354  0.22958146  0.82813313 ... -1.56607354 -0.36897021\n",
      "   0.82813313]\n",
      " [-0.87582323  0.62853197  0.62853197 ... -0.87582323 -0.87582323\n",
      "  -0.87582323]\n",
      " [-1.08665717  0.03514285 -0.18094158 ... -2.16248178  0.01675268\n",
      "   0.77534696]]\n"
     ]
    }
   ],
   "source": [
    "train_X = standardize(train_X)\n",
    "print(\"Standardize train_X : \"+str(train_X.shape)+\"\\n\\n\"+str(train_X))\n",
    "test_X  = standardize(test_X)\n",
    "print(\"\\n\\nStandardize test_X : \"+str(test_X.shape)+\"\\n\\n\"+str(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigz= 1/(1+np.exp(-Z))\n",
    "    sigz[sigz==1] = 0.99999999999\n",
    "    sigz[sigz==0] = 0.000000000001\n",
    "    return sigz        \n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.1\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(2/layer_dims[l-1])**0.5\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    #Z = standardize(Z) Batch-Normalize with u,var=1\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation,layer):\n",
    "    \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "        dropout_cache = A\n",
    "        D = np.random.rand(A.shape[0],A.shape[1]) \n",
    "        if layer==1:\n",
    "            D[:,:]=1\n",
    "        else:\n",
    "            D = (D < keep_prob).astype(int)                                         \n",
    "            A = A*D                                         \n",
    "            A = A/keep_prob \n",
    "        global Dcache \n",
    "        Dcache = D\n",
    "    \n",
    "    cache = (linear_cache, activation_cache,Dcache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    D = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)],\"relu\",l)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)],\"sigmoid\",l)\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache,keep_prob):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation,keep_prob):\n",
    "\n",
    "    linear_cache, activation_cache, dropout_cache = cache\n",
    "    global dA_prev, dW, db\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob=1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "   \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.003, num_iterations = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_after=1):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 10000000\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        t = t + 0.1\n",
    "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        if i % print_after == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if  i % print_after == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per '+str(print_after)+')')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    #print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    error = (1-acc)*100\n",
    "    print(Set+\" :       \"+'%0.3f'%error+\" %\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.800918\n",
      "Cost after iteration 100: 0.393619\n",
      "Cost after iteration 200: 0.259920\n",
      "Cost after iteration 300: 0.219754\n",
      "Cost after iteration 400: 0.205682\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :       9.075 %\n",
      "Test  :       9.589 %\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "global dropout_cache\n",
    "global keep_prob\n",
    "keep_prob=1\n",
    "parameters = model(train_X, train_Y, layers_dims = [10,65,50,25,15,10,5,1], num_iterations =500, \n",
    "                           learning_rate = 0.000085,  beta1 = 0.92, beta2 = 0.99,  epsilon = 1e-8, \n",
    "                           print_after = 100)\n",
    "\n",
    "def predict(X,parameters):\n",
    "    keep_prob=1\n",
    "    AL = forwardprop(X, parameters)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,parameters)\n",
    "train_Yhat = predict(train_X,parameters)\n",
    "\n",
    "\n",
    "#print(\"    \"+\" :       \"+ \"\\t Precision \" + \"  \"+ \"     \\tRecall\" +\"  \"+\"          F-score \"+\"  \"+\"         Accuracy\")\n",
    "\n",
    "evaluate(train_Y,train_Yhat,\"Train\")\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #print(caches[-2][-1].shape)\n",
    "    #print(L)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\",keep_prob=1)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        global Dprev_cache\n",
    "        D_prev = caches[l-1][2]\n",
    "        global dA_prev_temp, dW_temp, db_temp\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                \"relu\",keep_prob)\n",
    "        if l > 0:\n",
    "            dA_prev_temp = np.multiply(dA_prev_temp,D_prev)\n",
    "            dA_prev_temp = dA_prev_temp/keep_prob\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
