{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('a2_data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[1. 1. 1. ... 1. 0. 0.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n",
      "49.86301369863014\n"
     ]
    }
   ],
   "source": [
    "#Removing Rank 1 array\n",
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))\n",
    "print((np.sum(Y)/1460)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "X = data_orig[:,0:-1].T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))\n",
    "print((np.sum(1-test_Y)/292)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigz= 1/(1+np.exp(-Z))\n",
    "    sigz[sigz==1] = 0.99999999999             #Incase of flow\n",
    "    sigz[sigz==0] = 0.000000000001\n",
    "    return sigz        \n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layers):\n",
    "    p = {}\n",
    "    L = len(layers)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        p['W' + str(l)] = np.random.randn(layers[l],layers[l-1])*(2/layers[l-1])**0.5\n",
    "        p['b' + str(l)] = np.zeros((layers[l],1))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    #Z = standardize(Z) Batch-Normalize with u,var=1\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation,layer):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "        dropout_cache = A\n",
    "        D = np.random.rand(A.shape[0],A.shape[1]) \n",
    "        if layer==1:\n",
    "            D[:,:]=1\n",
    "        else:\n",
    "            D = (D < keep_prob).astype(int)                                         \n",
    "            A = A*D                                         \n",
    "            A = A/keep_prob \n",
    "        global Dcache \n",
    "        Dcache = D\n",
    "    \n",
    "    cache = (linear_cache, activation_cache,Dcache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, p):\n",
    "\n",
    "    caches = []\n",
    "    D = []\n",
    "    A = X\n",
    "    L = len(p) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, p[\"W\"+str(l)], p[\"b\"+str(l)],\"relu\",l)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, p[\"W\"+str(L)], p[\"b\"+str(L)],\"sigmoid\",l)\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y,p):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    sumW = 0\n",
    "    L = len(p) // 2 \n",
    "    for l in range(1, L):\n",
    "        sumW= sumW + np.sum(np.square(p[\"W\"+str(l)]))\n",
    "        \n",
    "    L2_cost= lambd*(sumW)/(2*m)\n",
    "    cost = cost + L2_cost\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache,keep_prob):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation,keep_prob):\n",
    "\n",
    "    linear_cache, activation_cache, dropout_cache = cache\n",
    "    global dA_prev, dW, db\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob=1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #print(caches[-2][-1].shape)\n",
    "    #print(L)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\",keep_prob=1)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        global Dprev_cache\n",
    "        D_prev = caches[l-1][2]\n",
    "        global dA_prev_temp, dW_temp, db_temp\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                \"relu\",keep_prob)\n",
    "        if l > 0:\n",
    "            dA_prev_temp = np.multiply(dA_prev_temp,D_prev)\n",
    "            dA_prev_temp = dA_prev_temp/keep_prob\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(p) :\n",
    "\n",
    "    L = len(p) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((p[\"W\" + str(l+1)].shape[0],p[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((p[\"b\" + str(l+1)].shape[0],p[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((p[\"W\" + str(l+1)].shape[0],p[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((p[\"b\" + str(l+1)].shape[0],p[\"b\" + str(l+1)].shape[1]))\n",
    "   \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_optimizer(p, g, v, s, t,m, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8,):\n",
    "\n",
    "    L = len(p) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*g['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*g['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(g['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(g['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        p[\"W\" + str(l+1)] = p[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        p[\"W\" + str(l+1)] = p[\"W\"+ str(l+1)] +(lambd/m)*p[\"W\" + str(l+1)]\n",
    "        p[\"b\" + str(l+1)] = p[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return p, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate0 = 0.003, epocs = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_after=1):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    t = 0\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    for i in range(0, epocs):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y,parameters)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        \n",
    "        t = t + 1\n",
    "        learning_rate = learning_rate0/(1+decay_rate*i)\n",
    "        \n",
    "        parameters, v, s = Adam_optimizer(parameters, grads, v, s,\n",
    "                                                               t,m, learning_rate, beta1, beta2,  epsilon,)\n",
    "        if i % print_after == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if  i % print_after == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per '+str(print_after)+')')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    #print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    data = [{'Precision': p, 'Recall': r, 'Accuracy': acc,'F-score': f1score}] \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    display(df)\n",
    "    error = (1-acc)*100\n",
    "    print(Set+\" :       \"+'%0.3f'%error+\" %\" +'\\t'+str(f1score))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.722761\n",
      "Cost after iteration 100: 0.629324\n",
      "Cost after iteration 200: 0.509269\n",
      "Cost after iteration 300: 0.404124\n",
      "Cost after iteration 400: 0.322703\n",
      "Cost after iteration 500: 0.269336\n",
      "Cost after iteration 600: 0.244067\n",
      "Cost after iteration 700: 0.231895\n",
      "Cost after iteration 800: 0.228874\n",
      "Cost after iteration 900: 0.228984\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.92517</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.928082</td>\n",
       "      <td>0.928328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall  Accuracy   F-score\n",
       "0    0.92517  0.931507  0.928082  0.928328"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  :       7.192 %\t0.9283276450511946\n",
      "Seed of initialization : 3\n"
     ]
    }
   ],
   "source": [
    "global dropout_cache\n",
    "global keep_prob\n",
    "global lambd\n",
    "\n",
    "np.random.seed(3)\n",
    "keep_prob=1\n",
    "lambd = 5e-2\n",
    "decay_rate=0\n",
    "\n",
    "p = model(train_X, train_Y, layers_dims = [10,64,32,16,8,4,1], epocs =901, \n",
    "                           learning_rate0 = 0.00009,  beta1 = 0.9, beta2 = 0.9,  epsilon = 1e-8, \n",
    "                           print_after = 100)\n",
    "keep_prob=1\n",
    "\n",
    "def predict(X,p):\n",
    "    AL = forwardprop(X, p)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,p)\n",
    "train_Yhat = predict(train_X,p)\n",
    "\n",
    "#evaluate(train_Y,train_Yhat,\"Train \")\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n",
    "print(\"Seed of initialization : \"+str(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
